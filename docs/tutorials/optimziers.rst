=========
Optimizer
=========




References
==========

.. [1] Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent
        learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society.
.. [2] Qian, N. (1999). On the momentum term in gradient descent learning algorithms.
        Neural Networks : The Official Journal of the International Neural Network Society,
        12(1), 145–151. http://doi.org/10.1016/S0893-6080(98)00116-6
.. [3] Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive Subgradient Methods for Online
        Learning and Stochastic Optimization. Journal of Machine Learning Research, 12,
        2121–2159. Retrieved from http://jmlr.org/papers/v12/duchi11a.html
.. [4] Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, … Ng, A. Y.
        (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information
        Processing Systems, 1–11. http://doi.org/10.1109/ICDAR.2011.95
.. [5] Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global Vectors for
        Word Representation. Proceedings of the 2014 Conference on Empirical Methods in
        Natural Language Processing, 1532–1543. http://doi.org/10.3115/v1/D14-1162
.. [6] Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from
        http://arxiv.org/abs/1212.5701
.. [7] Nesterov, Y. (1983). A method for unconstrained convex minimization problem with
        the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.),
         vol. 269, pp. 543– 547.
.. [8] Bengio, Y., Boulanger-Lewandowski, N., & Pascanu, R. (2012). Advances in Optimizing
        Recurrent Networks. Retrieved from http://arxiv.org/abs/1212.0901
.. [9] Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis.
.. [10] Darken, C., Chang, J., & Moody, J. (1992). Learning rate schedules for faster
        stochastic gradient search. Neural Networks for Signal Processing II Proceedings
        of the 1992 IEEE Workshop, (September), 1–11. http://doi.org/10.1109/NNSP.1992.253713
.. [11] H. Robinds and S. Monro, “A stochastic approximation method,” Annals of Mathematical
        Statistics, vol. 22, pp. 400–407, 1951.
.. [12] Mcmahan, H. B., & Streeter, M. (2014). Delay-Tolerant Algorithms for Asynchronous
        Distributed Online Learning. Advances in Neural Information Processing Systems
        (Proceedings of NIPS), 1–9. Retrieved from http://papers.nips.cc/paper/5242-delay-
        tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf
.. [13] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., … Zheng, X.
        (2015). TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed
        Systems.
.. [14] Zhang, S., Choromanska, A., & LeCun, Y. (2015). Deep learning with Elastic
        Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015), 1–24.
        Retrieved from http://arxiv.org/abs/1412.6651
.. [15] Kingma, D. P., & Ba, J. L. (2015). Adam: a Method for Stochastic Optimization.
        International Conference on Learning Representations, 1–13.
.. [16] Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning.
        Proceedings of the 26th Annual International Conference on Machine Learning, 41–48.
        http://doi.org/10.1145/1553374.1553380
.. [17] Zaremba, W., & Sutskever, I. (2014). Learning to Execute, 1–25. Retrieved from
        http://arxiv.org/abs/1410.4615
.. [18] Ioffe, S., & Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network
        Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3.
.. [19] Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014).
        Identifying and attacking the saddle point problem in high-dimensional non-convex
        optimization. arXiv, 1–14. Retrieved from http://arxiv.org/abs/1406.2572
.. [20] Sutskever, I., & Martens, J. (2013). On the importance of initialization and
        momentum in deep learning. http://doi.org/10.1109/ICASSP.2013.6639346
.. [21] Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K.,
        & Martens, J. (2015). Adding Gradient Noise Improves Learning for Very Deep
        Networks, 1–11. Retrieved from http://arxiv.org/abs/1511.06807
.. [22] LeCun, Y., Bottou, L., Orr, G. B., & Müller, K. R. (1998). Efficient BackProp.
        Neural Networks: Tricks of the Trade, 1524, 9–50. http://doi.org/10.1007/3-
        540-49430-8_2
.. [23] Niu, F., Recht, B., Christopher, R., & Wright, S. J. (2011). Hogwild! : A
        Lock-Free Approach to Parallelizing Stochastic Gradient Descent, 1–22.
.. [24] Duchi et al. [3] give this matrix as an alternative to the full matrix containing
        the outer products of all previous gradients, as the computation of the matrix
        square root is infeasible even for a moderate number of parameters dd.

